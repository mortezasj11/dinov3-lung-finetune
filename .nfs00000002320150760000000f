#!/bin/bash
################################################################################
# Script: log-runner.sh
# Generated on Fri Sep 26 10:46:35 AM CDT 2025.

# Set Vars
LRBASEDIR="/rsrch7/wulab/Lung_Foundation_Model_Data_/Down-stream_tasks/Histology/FineTune/code_1gpu_loss_wrapper_muex"
LRLOGDIR="/rsrch7/wulab/Lung_Foundation_Model_Data_/Down-stream_tasks/Histology/FineTune/code_1gpu_loss_wrapper_muex/logs"
LRKUBECONFIG="/home/msalehjahromi/.kube/config"
LRJOBNAME="msalehjahromi-histology-mnt-22"
LRCONTAINERNAME="main"
LRNAMESPACE="yn-gpu-workload"
LRLOGFILE="/rsrch7/wulab/Lung_Foundation_Model_Data_/Down-stream_tasks/Histology/FineTune/code_1gpu_loss_wrapper_muex/logs/msalehjahromi-histology-mnt-22-runner-2025-09-26_104604.log"
LRCONTAINERLOGFILE="/rsrch7/wulab/Lung_Foundation_Model_Data_/Down-stream_tasks/Histology/FineTune/code_1gpu_loss_wrapper_muex/logs/msalehjahromi-histology-mnt-22-main-2025-09-26_104604.log"
LRJOBDESCRIBELOGFILE="/rsrch7/wulab/Lung_Foundation_Model_Data_/Down-stream_tasks/Histology/FineTune/code_1gpu_loss_wrapper_muex/logs/msalehjahromi-histology-mnt-22-describe-2025-09-26_104604.log"
MyUser="msalehjahromi"

# Logging that the log-runner has started.
echo -e "$(date +'%F_%H%M%S'): The log-runner.sh script for job/${LRJOBNAME} is now running." >> ${LRLOGFILE}

# Check if Job is Running.
JobStatus="$(kubectl --kubeconfig="${LRKUBECONFIG}" -n "${LRNAMESPACE}" get job "${LRJOBNAME}" >/dev/null 2>&1; echo $?)"

while [ "${JobStatus}" -eq "0" ];do

    # Set the interval that the loop will recheck the log tailer status.
    sleep 30
	
    # Recheck if Job is Running
    JobStatus="$(kubectl --kubeconfig="${LRKUBECONFIG}" -n "${LRNAMESPACE}" get job "${LRJOBNAME}" >/dev/null 2>&1; echo $?)"

    if [[ "${JobStatus}" -gt "0" ]]
    then
        # Logging that the job is no longer running.
        echo -e "$(date +'%F_%H%M%S'): The job/${LRJOBNAME} is no longer running. Log-runner.sh script is stopping." >> ${LRLOGFILE}
        touch ${LRBASEDIR}/done
        rm -f ${LRBASEDIR}/.log-runner.sh > /dev/null 2>&1
		exit 0
    fi

    # Dumping the description of the job being monitored.
    $(kubectl --kubeconfig="${LRKUBECONFIG}" -n "${LRNAMESPACE}" describe job "${LRJOBNAME}" >${LRJOBDESCRIBELOGFILE} 2>&1)

    # Check if there is already a process logging the container log.
    # This checks for a kubectl logs process that matches the job & namespace is actively running.
    # Could look to enhance the accuracy of the script by comparing the returned PID against the status of any associated PID with the open container log file. Keeping simple for now.
    loggerStatus="$(ps -f -u ${MyUser} | grep "kubectl" | grep ""${LRNAMESPACE}" "job/${LRJOBNAME}"" | grep -v "grep" >/dev/null 2>&1; echo $?)"

    if ! [[ "${loggerStatus}" -ge "1" ]]
    then
        # Kubectl log tailer process found
        loggerCount="$(ps -f -u ${MyUser} | grep "kubectl" | grep "$LRNAMESPACE job/$LRJOBNAME" |grep -v 'grep' | wc -l)"

        if [[ "${loggerCount}" -eq "1" ]]
        then
            # Touching the job-runner.sh log file to update the timestamp so you can tell the log-runner.sh script hasn't hung.
            $(touch ${LRLOGFILE})
        fi

    else
        echo -e "$(date +'%F_%H%M%S'): Kubectl Log tailer for job/${LRJOBNAME} appears to no longer be running." >> ${LRLOGFILE}
        echo -e "$(date +'%F_%H%M%S'): Checking the current state of job/${LRJOBNAME}." >> ${LRLOGFILE}

        # Get State of job:
        # JobStateCheck1 checks if the job is currently 'active' (equates to running / not complete).
        # JobStateCheck2 checks if the job is currently 'complete'.
        JobStateCheck1="$(kubectl --kubeconfig="${LRKUBECONFIG}" -n "${LRNAMESPACE}" get job "${LRJOBNAME}" -o=jsonpath='{.status}'|grep '"active":1' >/dev/null 2>&1; echo $? )"
        JobStateCheck2="$(kubectl --kubeconfig="${LRKUBECONFIG}" -n "${LRNAMESPACE}" get job "${LRJOBNAME}" -o=jsonpath='{.status}'|grep '"status":"True","type":"Complete"' >/dev/null 2>&1; echo $?)"

        if [ "${JobStateCheck1}" -eq "0" -a "${JobStateCheck2}" -eq "1" ]
        then
            echo -e "$(date +'%F_%H%M%S'): Job/${LRJOBNAME} appears to still be active.  Starting the logger again." >> ${LRLOGFILE}
            nohup kubectl --kubeconfig="${LRKUBECONFIG}" logs -n ${LRNAMESPACE} job/${LRJOBNAME} -f --all-containers --pod-running-timeout=30s --prefix --timestamps --ignore-errors=true >> ${LRCONTAINERLOGFILE} 2>&1 &

            # Dumping the description of the job & pods being monitored.
            # Goal here is to try and capture all of the current pods (starting, running, terminating, error, ect) at this time.
            $(kubectl --kubeconfig="${LRKUBECONFIG}" -n "${LRNAMESPACE}" describe jobs,pods -l "job-name=${LRJOBNAME}" >>"${LRLOGDIR}/${LRJOBNAME}-restarted-describe-$(date +'%F_%H%M%S').log" 2>&1)

        else
            if [ "${JobStateCheck1}" -eq "1" -a "${JobStateCheck2}" -eq "0" ]
            then
                echo -e "$(date +'%F_%H%M%S'): Job/${LRJOBNAME} appears to have completed.  Log-runner.sh script is stopping." >> ${LRLOGFILE}
                # Dumping the description of the job & pods being monitored.
                # Goal here is to try and capture all of the current pods (starting, running, terminating, error, ect) at this time.
                $(kubectl --kubeconfig="${LRKUBECONFIG}" -n "${LRNAMESPACE}" describe jobs,pods -l "job-name=${LRJOBNAME}" >>"${LRLOGDIR}/${LRJOBNAME}-completion-describe-$(date +'%F_%H%M%S').log" 2>&1)	
                touch ${LRBASEDIR}/done
                rm -f ${LRBASEDIR}/.log-runner.sh > /dev/null 2>&1
                JobStatus="1"
            fi	
        fi
    fi
done
